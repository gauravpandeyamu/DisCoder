{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotting\n",
    "#import cv2\n",
    "from torch.nn.utils import weight_norm\n",
    "import scipy.misc\n",
    "#%matplotlib inline\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# to prevent opencv from initializing CUDA in workers\n",
    "#torch.randn(8).cuda()\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "def gallery(array, ncols=3):\n",
    "    nindex, height, width, intensity = array.shape\n",
    "    nrows = nindex//ncols\n",
    "    assert nindex == nrows*ncols\n",
    "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
    "    result = (array.reshape((nrows, ncols, height, width, intensity))\n",
    "              .swapaxes(1,2)\n",
    "              .reshape((height*nrows, width*ncols, intensity)))\n",
    "    return result\n",
    "\n",
    "def save_samples():\n",
    "    img_bhwc = netG(noise).data.cpu().add_(1).mul_(.5)\n",
    "    img_bhwc = img_bhwc.permute(0,2,3,1).numpy()\n",
    "    array = img_bhwc.copy()\n",
    "    result = gallery(array,10)*.5+.5\n",
    "    scipy.misc.imsave('svhn_noent_6.jpg', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 100\n",
    "learning_rate = .0003\n",
    "batch_size = 100\n",
    "unlabeled_weight = 1\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('.', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,)  )\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('.', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "trainx = torch.from_numpy(train_loader.dataset.train_data).float().permute(0,3,1,2)/255\n",
    "trainy = torch.from_numpy(np.array(train_loader.dataset.train_labels))\n",
    "trainx.add_(-.5).mul_(2);\n",
    "\n",
    "trainx_unl = trainx.clone()\n",
    "trainx_unl2 = trainx.clone()\n",
    "nr_batches_train = int(trainx.size(0)/batch_size)\n",
    "\n",
    "\n",
    "testx = torch.from_numpy(test_loader.dataset.test_data).float().permute(0,3,1,2)/255\n",
    "testy = torch.from_numpy(np.array(test_loader.dataset.test_labels))\n",
    "testx.add_(-.5).mul_(2);\n",
    "nr_batches_test = int(testx.size(0)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_dim = (batch_size, 100)\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 8 x 8\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 16 x 16\n",
    "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear')!=-1:\n",
    "        m.weight.data.normal_(0.0, 0.05)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    \n",
    "    \n",
    "netG = _netG()\n",
    "netG.cuda()\n",
    "netG.apply(weights_init);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DynamicGNoise(nn.Module):\n",
    "    def __init__(self, shape, std=0.05):\n",
    "        super().__init__()\n",
    "        self.noise = Variable(torch.zeros(1,1,shape,shape).cuda())\n",
    "        self.std   = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not training: return x\n",
    "        self.noise.data.normal_(0, std=self.std)      \n",
    "        return x + self.noise.expand_as(x)\n",
    "\n",
    "ins = [3,64,64,64,128,128,128,128,128]\n",
    "outs = [64,64,64,128,128,128,128,128,128]\n",
    "filters = [3,3,3,3,3,3,3,1,1]\n",
    "strides = [1,1,2,1,1,2,1,1,1]\n",
    "pads = [1,1,1,1,1,1,0,0,0]\n",
    "  \n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "\n",
    "        self.conv = [None]*9\n",
    "        for i in range(0,9):\n",
    "            self.conv[i] = nn.Conv2d(ins[i],outs[i],filters[i],strides[i],pads[i])\n",
    "            self.conv[i].weight.data.normal_(0.0, 0.05)\n",
    "            self.conv[i].bias.data.fill_(0)\n",
    "            self.conv[i] = weight_norm(self.conv[i])\n",
    "        self.conv = nn.ModuleList(self.conv)\n",
    "        self.linear = nn.Linear(128,10)\n",
    "        self.linear.weight.data.normal_(0.0, 0.05)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        self.linear = weight_norm(self.linear)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(.2)              \n",
    "        self.drop2 = nn.Dropout(.5)      \n",
    "        self.drop3 = nn.Dropout(.5)\n",
    "\n",
    "        self.global_pool = nn.AvgPool2d(6)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop1(x)\n",
    "        for i in range(0,3):\n",
    "            x = self.conv[i](x)\n",
    "            x = F.leaky_relu(x, negative_slope=.2)\n",
    "\n",
    "        x = self.drop2(x)\n",
    "        for i in range(3,6):\n",
    "            x = self.conv[i](x)\n",
    "            x = F.leaky_relu(x, negative_slope=.2)\n",
    "\n",
    "        x = self.drop3(x)\n",
    "        for i in range(6,9):\n",
    "            x = self.conv[i](x)\n",
    "            x = F.leaky_relu(x, negative_slope=.2)\n",
    "        \n",
    "        features = self.global_pool(x).view(-1,128)\n",
    "        x = self.linear(features)\n",
    "\n",
    "        return x, features\n",
    "\n",
    "netD = _netD()\n",
    "netD.cuda()\n",
    "#netD.apply(weights_init);\n",
    "\n",
    "optimizerC = optim.Adam(netD.parameters(), lr=learning_rate, betas=(.5, .999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(.5, .999))\n",
    "noise = Variable(torch.randn(batch_size,100,1,1).cuda())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.cuda()\n",
    "phi_all = torch.rand(3000,10).cuda()\n",
    "phi_all /= phi_all.sum(1).unsqueeze(1).expand_as(phi_all)\n",
    "classMat = torch.eye(10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(x_unl):\n",
    "    global avg, saved_g\n",
    "    netD.eval()\n",
    "    x = Variable(x_unl.cuda())\n",
    "    for i in range(0,9):\n",
    "        x_new = netD.conv[i](x).data\n",
    "        m = x_new.mean(3).mean(2).mean(0)\n",
    "        inv_stdv = 1/(x_new**2).mean(3).mean(2).mean(0).sqrt().view(-1,1,1,1)\n",
    "        netD.conv[i].weight_g.data.copy_(netD.conv[i].weight_g.data*inv_stdv)\n",
    "        netD.conv[i].bias.data.copy_(-m*inv_stdv.squeeze())\n",
    "        x = netD.conv[i](x)\n",
    "        x = F.leaky_relu(x, negative_slope=.2)\n",
    "\n",
    "    x = netD.global_pool(x).view(-1,128)        \n",
    "    x_new = netD.linear(x).data\n",
    "    m = x_new.mean(0).squeeze()\n",
    "    inv_stdv = .1/(x_new**2).mean(0).squeeze().sqrt().view(-1,1)\n",
    "    netD.linear.weight_g.data.copy_(netD.linear.weight_g.data*inv_stdv)\n",
    "    netD.linear.bias.data.copy_(-m*inv_stdv.squeeze())\n",
    "    \n",
    "    avg = [None]*30\n",
    "    i = 0\n",
    "    for param in netD.parameters():\n",
    "        avg[i] = param.data.clone()\n",
    "        i += 1\n",
    "\n",
    "    saved_g = [None]*9\n",
    "    for i in range(0,9):\n",
    "        saved_g[i] = netD.conv[i].weight_g.data.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior = torch.zeros(10).cuda()\n",
    "for i in range(0,10):\n",
    "    prior[i] = 0.1\n",
    "prior = prior.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Estep(phi):\n",
    "    if(epoch==0):\n",
    "        px_z = phi/phi.sum(0).expand_as(phi)\n",
    "    else:\n",
    "        px_z = phi/phi_all.sum(0).expand_as(phi)\n",
    "    px_z *= prior\n",
    "    _, inds = px_z.max(1)\n",
    "    z = classMat.index_select(0, inds.squeeze())\n",
    "    #z = px_z**2\n",
    "    #z /= z.sum(1, keepdim=True).expand_as(z)\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(x_lab, labels, x_unl):\n",
    "    global phi_all\n",
    "    netD.train()\n",
    "    \n",
    "    optimizerC.zero_grad()\n",
    "    labels = Variable(labels.cuda())\n",
    "    x_lab = Variable(x_lab.cuda())\n",
    "    output_before_softmax_lab = netD(x_lab)[0]\n",
    "    loss_lab = loss_fn(output_before_softmax_lab, labels)\n",
    "    \n",
    "    x_unl = Variable(x_unl.cuda())\n",
    "    output_before_softmax_unl = netD(x_unl)[0]\n",
    "    output_after_softmax_unl = F.softmax(output_before_softmax_unl)\n",
    "    phi_all = torch.cat((output_after_softmax_unl.data, phi_all),0)[0:3000]\n",
    "    z = Estep(output_after_softmax_unl.data)\n",
    "    z = Variable(z)\n",
    "    log_phi = torch.log(output_after_softmax_unl+1e-5)\n",
    "    \n",
    "    exponent = torch.mm(z, log_phi.t())\n",
    "    exponent2 = exponent - torch.diag(exponent).view(batch_size,1).expand_as(exponent)\n",
    "    temp = exponent2.exp()\n",
    "    px_z_inv = temp.sum(1)\n",
    "    loss_unl = px_z_inv.log().mean()\n",
    "\n",
    "    \n",
    "    noise.data.normal_(0,1)\n",
    "    gen_data = netG(noise)\n",
    "    output_before_softmax_gen = netD(gen_data.detach())[0]\n",
    "    output_after_softmax_gen = F.softmax(output_before_softmax_gen)\n",
    "    loss_gen = (torch.log(output_after_softmax_gen+1e-5)).mean(1).mean()*-1\n",
    "    \n",
    "\n",
    "    loss = loss_lab + loss_unl + loss_gen\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizerC.step()\n",
    "    train_err = (output_before_softmax_lab.data.max(1)[1]==labels.data).sum()/batch_size\n",
    "    return train_err, loss_lab.data[0], loss_unl.data[0], loss_gen.data[0]\n",
    "    \n",
    "\n",
    "def test_classifier(x_test, labels):\n",
    "    netD.eval()\n",
    "    x_test = Variable(x_test.cuda())\n",
    "    output_before_softmax = netD(x_test)\n",
    "    test_err = (output_before_softmax.data.max(1)[1]==labels).sum()/batch_size\n",
    "    return test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator(x_unl):\n",
    "    netD.train()\n",
    "\n",
    "    optimizerG.zero_grad()\n",
    "    x_unl = Variable(x_unl.cuda())\n",
    "    noise.data.normal_(0,1)\n",
    "    gen_data = netG(noise)\n",
    "    output_unl = netD(x_unl)[1]\n",
    "    output_gen = netD(gen_data)[1]\n",
    "    m1 = output_unl.mean(0)\n",
    "    m2 = output_gen.mean(0)\n",
    "    loss_gen = (m1-m2).abs().mean()\n",
    "    loss_gen.backward()\n",
    "    optimizerG.step()\n",
    "    \n",
    "    return loss_gen.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select labeled data\n",
    "shuffle = torch.randperm(trainx.size(0))\n",
    "trainx = trainx[shuffle]\n",
    "trainy = trainy[shuffle]\n",
    "count = 4000\n",
    "txs = torch.zeros(count*8,3,32,32)\n",
    "tys = torch.zeros(count*8)\n",
    "for clas in range(2,10):\n",
    "    i = clas - 2\n",
    "    inds = trainy.eq(clas).nonzero()[0:count]\n",
    "    txs[i*count:(i+1)*count,:,:,:] = trainx.index_select(0, inds.squeeze())\n",
    "    tys[i*count:(i+1)*count] = trainy.index_select(0, inds.squeeze())\n",
    "\n",
    "\n",
    "count = 4000    \n",
    "trainx_unl = torch.zeros(count*2,3,32,32)   \n",
    "trainy_unl = torch.zeros(count*2)   \n",
    "for clas in range(0,2):\n",
    "    i = clas\n",
    "    inds = trainy.eq(clas).nonzero()[0:count]\n",
    "    trainx_unl[i*count:(i+1)*count,:,:,:] = trainx.index_select(0, inds.squeeze())\n",
    "    trainy_unl[i*count:(i+1)*count] = trainy.index_select(0, inds.squeeze())\n",
    "\n",
    "count = 400\n",
    "testx1 = torch.zeros(count*2,3,32,32)   \n",
    "testy1 = torch.zeros(count*2)   \n",
    "for clas in range(0,2):\n",
    "    i = clas \n",
    "    inds = trainy.eq(clas).nonzero()[count:2*count]\n",
    "    testx1[i*count:(i+1)*count,:,:,:] = trainx.index_select(0, inds.squeeze())\n",
    "    testy1[i*count:(i+1)*count] = trainy.index_select(0, inds.squeeze())\n",
    "    \n",
    "\n",
    "trainx_unl2 = torch.cat((trainx_unl,txs),0)\n",
    "trainx_unl = trainx_unl2.clone()\n",
    "    \n",
    "import time\n",
    "import scipy.misc\n",
    "\n",
    "training = True\n",
    "train_err = torch.zeros(900)\n",
    "loss_lab = torch.zeros(900)\n",
    "loss_unl = torch.zeros(900)\n",
    "loss_gen = torch.zeros(900)\n",
    "lossG_gen = torch.zeros(900)\n",
    "accuracy = torch.zeros(900)\n",
    "\n",
    "scale = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in range(0,900):\n",
    "    \n",
    "        \n",
    "    shuffle = torch.randperm(txs.size(0))\n",
    "    txs = txs[shuffle]\n",
    "    tys = tys[shuffle]\n",
    "    \n",
    "    shuffle = torch.randperm(trainx_unl.size(0))\n",
    "    trainx_unl = trainx_unl[shuffle]\n",
    "\n",
    "    trainx_unl2 = trainx_unl2[torch.randperm(trainx_unl2.size(0))]\n",
    "\n",
    "    if epoch==0:\n",
    "        print(trainx.shape)\n",
    "        initialize(trainx[:500]) # data based initialization\n",
    "\n",
    "    \n",
    "    \n",
    "    numBatches = 0\n",
    "    N = txs.size(0) - txs.size(0)%batch_size\n",
    "    for i in range(0, N, batch_size):\n",
    "        numBatches +=1\n",
    "        x_lab = txs[i:i+batch_size]\n",
    "        labels = tys[i:i+batch_size].long()\n",
    "        x_unl = trainx_unl[i:i+batch_size]\n",
    "        te, ll, lu, lg = train_classifier(x_lab, labels, x_unl)\n",
    "        train_err[epoch] += te\n",
    "        loss_lab[epoch] += ll\n",
    "        loss_unl[epoch] += lu\n",
    "        loss_gen[epoch] += lg\n",
    "        \n",
    "        x_unl = trainx_unl2[i:i+batch_size]\n",
    "        lgg = train_generator(x_unl)\n",
    "        lossG_gen[epoch] += lgg\n",
    "        j=0\n",
    "        for param in netD.parameters():\n",
    "            avg[j] = avg[j] + .0001*(param.data - avg[j])\n",
    "            j += 1 \n",
    "        for j in range(0,9):\n",
    "            netD.conv[j].weight_g.data.copy_(saved_g[j])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    " \n",
    "    \n",
    "     #Computation of purity on the first two clusters\n",
    "    posterior = torch.zeros(testx1.size(0), 2)\n",
    "    netD.eval()\n",
    "    training = False\n",
    "    for i in range(0, testx1.size(0), batch_size):\n",
    "        real_cpu = testx1[i:i+batch_size].clone()\n",
    "        input = Variable(real_cpu.cuda())\n",
    "        output = netD(input)[0]\n",
    "        posterior[i:i+batch_size] = output.data.cpu().clone()[:,:2] \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    _, indices_fake = posterior.cpu().max(1)\n",
    "    indices_fake = indices_fake.squeeze().float()\n",
    "    indices_real = testy1\n",
    "    intersect = torch.zeros(2,2)\n",
    "    for i in range(0,2):\n",
    "        for j in range(0,2):\n",
    "            intersect[i][j] = ((indices_fake==i)*(indices_real==j)).sum()\n",
    "\n",
    "    accuracy[epoch] = intersect.max(1)[0].sum()/testx1.size(0)\n",
    "    print(\"epoch:%d, loss_lab:%.4f, loss_gen:%.4f, loss_unl:%.4f, lossG_gen:%.4f, train_err:%.4f, test_err:%.4f\" % (epoch, loss_lab[epoch]/numBatches, loss_gen[epoch]/numBatches, \n",
    "          loss_unl[epoch]/numBatches, lossG_gen[epoch]/numBatches, train_err[epoch]/numBatches, accuracy[epoch]))\n",
    "\n",
    "    \n",
    "    save_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    img_bhwc = netG(noise).data.cpu().add_(1).mul_(.5)\n",
    "    img_bhwc = img_bhwc.permute(0,2,3,1).numpy()\n",
    "    img_tile = plotting.img_tile(img_bhwc, aspect_ratio=1.0, border_color=1.0, stretch=True)\n",
    "    img = plotting.plot_img(img_tile, title='CIFAR10 samples')\n",
    "    plotting.plt.savefig(\"cifar_sample_feature_match.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(i/(i+batch_size)*batch_size, loss_lab/(i+batch_size)*batch_size,\n",
    "      loss_gen/(i+batch_size)*batch_size, loss_unl/(i+batch_size)*batch_size,\n",
    "      train_err/(i+batch_size)*batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_bhwc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
