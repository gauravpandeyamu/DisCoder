{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotting\n",
    "#import cv2\n",
    "from torch.nn.utils import weight_norm\n",
    "import scipy.misc\n",
    "#%matplotlib inline\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# to prevent opencv from initializing CUDA in workers\n",
    "#torch.randn(8).cuda()\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "def gallery(array, ncols=3):\n",
    "    nindex, height, width, intensity = array.shape\n",
    "    nrows = nindex//ncols\n",
    "    assert nindex == nrows*ncols\n",
    "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
    "    result = (array.reshape((nrows, ncols, height, width, intensity))\n",
    "              .swapaxes(1,2)\n",
    "              .reshape((height*nrows, width*ncols, intensity)))\n",
    "    return result\n",
    "\n",
    "def save_samples():\n",
    "    img_bhwc = netG(noise).data.cpu()[0:100].view(100,1,28,28).expand(100,3,28,28)\n",
    "    img_bhwc = img_bhwc.permute(0,2,3,1).numpy()\n",
    "    array = img_bhwc.copy()\n",
    "    result = gallery(array,10)*.5+.5\n",
    "    scipy.misc.imsave('outfile_noent_1.jpg', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 400\n",
    "learning_rate = .0001\n",
    "batch_size = 500\n",
    "unlabeled_weight = 1\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,)  )\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainx = train_loader.dataset.train_data.float().view(-1,1,28,28)/255\n",
    "trainy = train_loader.dataset.train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc = 1\n",
    "ndf = 64\n",
    "ncl = 100\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            #nn.Conv2d(ndf, ndf, 3, 1, 1, bias=False),\n",
    "            #nn.BatchNorm2d(ndf),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, ncl, 4, 1, 0, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return F.softmax(output.view(-1, ncl))\n",
    "\n",
    "\n",
    "netD = _netD()\n",
    "netD.cuda()\n",
    "checkpoint = torch.load('model')\n",
    "netD.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = torch.zeros(60000,100)\n",
    "for i in range(0, trainx.size(0), batch_size):\n",
    "    input = trainx[i:i+batch_size]\n",
    "    input = Variable(input.cuda())\n",
    "    posterior[i:i+batch_size] = netD(input).data.cpu()\n",
    "\n",
    "assignment_x = posterior.max(1)[1]\n",
    "intersect = torch.zeros(100,10)\n",
    "\n",
    "for i in range(0,100):\n",
    "    for j in range(0,10):\n",
    "        intersect[i][j] = ((assignment_x==i)*(trainy==j)).sum()\n",
    "\n",
    "accuracy = intersect.max(1)[0].sum()/trainx.size(0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncl = 160\n",
    "class DynamicGNoiseConv(nn.Module):\n",
    "    def __init__(self, shape, std=0.05):\n",
    "        super().__init__()\n",
    "        self.noise = Variable(torch.zeros(1,1,shape,shape).cuda())\n",
    "        self.std   = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not training: return x\n",
    "        self.noise.data.normal_(0, std=self.std)      \n",
    "        return x + self.noise.expand_as(x)\n",
    "\n",
    "class DynamicGNoise(nn.Module):\n",
    "    def __init__(self, shape, std=0.1):\n",
    "        super().__init__()\n",
    "        self.noise = Variable(torch.zeros(batch_size,shape).cuda())\n",
    "        self.std   = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not training: return x\n",
    "        self.noise.data.normal_(0, std=self.std)      \n",
    "        return x + self.noise\n",
    "    \n",
    "    \n",
    "ins = [784,1000,500,250,250,250,10]\n",
    "sigma = [.3,.5,.5,.5,.5,.5]\n",
    "  \n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,50,5)\n",
    "        self.conv1.weight.data.normal_(0,.05)\n",
    "        self.conv1.bias.data.fill_(0)\n",
    "        self.conv1 = weight_norm(self.conv1)\n",
    "        self.conv2 = nn.Conv2d(50,50,5)\n",
    "        self.conv2.weight.data.normal_(0,.05)\n",
    "        self.conv2.bias.data.fill_(0)\n",
    "        self.conv2 = weight_norm(self.conv2)\n",
    "        self.linear1 = nn.Linear(50*64,ncl)\n",
    "        self.linear1.weight.data.normal_(0,.05)\n",
    "        self.linear1.bias.data.fill_(0)\n",
    "        self.linear1 = weight_norm(self.linear1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(input=x, kernel_size=2, stride=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1,50*64)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "netD = _netD()\n",
    "netD.cuda()\n",
    "#netD.apply(weights_init);\n",
    "\n",
    "optimizerC = optim.Adam(netD.parameters(), lr=learning_rate, betas=(.5, .999))\n",
    "phi_all = torch.randn(6000,ncl).cuda()\n",
    "phi_all = Variable(phi_all)\n",
    "\n",
    "\n",
    "\n",
    "def initialize(x_unl):\n",
    "    global saved_g1, saved_g2\n",
    "    netD.eval()\n",
    "    x = Variable(x_unl.cuda())\n",
    "    x_new = netD.conv1(x).data\n",
    "    m = x_new.mean(3).mean(2).mean(0)\n",
    "    inv_stdv = 1/(x_new**2).mean(3).mean(2).mean(0).sqrt().view(-1,1,1,1)\n",
    "    netD.conv1.weight_g.data.copy_(netD.conv1.weight_g.data*inv_stdv)\n",
    "    netD.conv1.bias.data.copy_(-m*inv_stdv.squeeze())\n",
    "    x = netD.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(input=x, kernel_size=2, stride=2)\n",
    "\n",
    "    x_new = netD.conv2(x).data\n",
    "    m = x_new.mean(3).mean(2).mean(0)\n",
    "    inv_stdv = 1/(x_new**2).mean(3).mean(2).mean(0).sqrt().view(-1,1,1,1)\n",
    "    netD.conv2.weight_g.data.copy_(netD.conv2.weight_g.data*inv_stdv)\n",
    "    netD.conv2.bias.data.copy_(-m*inv_stdv.squeeze())\n",
    "    x = netD.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = x.view(-1,50*64)\n",
    "    \n",
    "    x_new = netD.linear1(x).data\n",
    "    m = x_new.mean(0)\n",
    "    inv_stdv = 1/(x_new**2).mean(0).sqrt().view(-1,1)\n",
    "    netD.linear1.weight_g.data.copy_(netD.linear1.weight_g.data*inv_stdv)\n",
    "    netD.linear1.bias.data.copy_(-m*inv_stdv.squeeze())\n",
    "    \n",
    "    \n",
    "    saved_g1 = netD.conv1.weight_g.data.clone()\n",
    "    saved_g2 = netD.conv2.weight_g.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(assignment_x, trainy):    \n",
    "    intersect = torch.zeros(max_class,10)\n",
    "    i = 0                           \n",
    "    for i in range(0, max_class):\n",
    "        for j in range(0,10):\n",
    "            intersect[i][j] = ((assignment_x==i)*(trainy==j)).sum()\n",
    "        i+=1    \n",
    "    accuracy = intersect.max(1)[0].sum()/intersect.sum()\n",
    "    return accuracy\n",
    "        \n",
    " \n",
    "def compress():\n",
    "    global assignment_x, max_class, emp_sum_k, count_k, mean_k\n",
    "    un_labs = torch.from_numpy(np.unique(assignment_x.cpu().numpy())).squeeze().long().cuda()\n",
    "    reversed_index = torch.zeros(un_labs.max()+1).long().cuda()\n",
    "    max_class = un_labs.size(0)\n",
    "    range1 = torch.from_numpy(np.asarray(range(0,max_class))).cuda()\n",
    "    reversed_index.index_copy_(0, un_labs, range1)\n",
    "    assignment_x = reversed_index.index_select(0,assignment_x.long())\n",
    "    emp_sum_k[:max_class] = emp_sum_k.index_select(0, un_labs)\n",
    "    count_k[:max_class] = count_k.index_select(0, un_labs)\n",
    "    means_k[:max_class] = emp_sum_k[:max_class]/(count_k[:max_class].unsqueeze(1) + rho0/rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_means(input, batchL):\n",
    "    count_k1.zero_()\n",
    "    emp_sum_k1.zero_()\n",
    "    for i in range(0, input.size(0)):\n",
    "        label = batchL[i]\n",
    "        emp_sum_k1[label] += input[i]\n",
    "        count_k1[label] += 1\n",
    "    \n",
    "    means_k[:max_class] = emp_sum_k1[:max_class]/(count_k1[:max_class].unsqueeze(1) + rho0/rho)\n",
    "    return means_k.index_select(0, batchL.cuda())\n",
    "\n",
    "def train_classifier(x_unl, batchL):\n",
    "    global phi_all, output\n",
    "    netD.train() \n",
    "    optimizerC.zero_grad()\n",
    "    \n",
    "    \n",
    "    x_unl = Variable(x_unl.cuda())\n",
    "    output = netD(x_unl)\n",
    "\n",
    "    #Computation of loss_disc\n",
    "    means = get_means(output.data, batchL)    \n",
    "    \n",
    "    z = output\n",
    "    phi_all = Variable(phi_all.data)\n",
    "    phi_all = torch.cat((output,phi_all),0)[0:6000]\n",
    "    term3 = torch.mm(z, phi_all.t())\n",
    "    term1 = (z**2).sum(1, keepdim=True).expand_as(term3)\n",
    "    term2 = (phi_all**2).sum(1, keepdim=True).t().expand_as(term3)\n",
    "    exponent = term1+term2-2*term3\n",
    "    exponent = rho/2*exponent\n",
    "    exponent = exponent*-1\n",
    "    temp = exponent.exp()\n",
    "    px_z_inv = temp.sum(1) \n",
    "    loss_disc = px_z_inv.log().mean() \n",
    "    means = Variable(means)\n",
    "    loss_prior = rho/2*((output - means)**2).sum(1).mean() \n",
    "    \n",
    "    loss = loss_disc + loss_prior\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizerC.step()\n",
    "    return loss_disc.data[0], loss_prior.data[0]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collapsedGibbs(input, batchL):\n",
    "    global max_class, probs\n",
    "    #Get the means based on the current assignments\n",
    "    #The mean is computed over the batch\n",
    "    smallest_ind = count_k[:max_class].min(0)[1][0]\n",
    "    if epoch<2 or max_class<100:\n",
    "        numEp = 10\n",
    "    else:\n",
    "        numEp = 3\n",
    "        \n",
    "    for iter in range(0,numEp):\n",
    "        count_k.zero_()\n",
    "        emp_sum_k.zero_()\n",
    "        for i in range(0, input.size(0)):\n",
    "            label = batchL[i]\n",
    "            if(label!=smallest_ind):\n",
    "                emp_sum_k[label] += input[i]\n",
    "                count_k[label] += 1\n",
    "\n",
    "        means_k = emp_sum_k[:max_class]/(count_k[:max_class].unsqueeze(1) + rho0/rho)\n",
    "        means_k[smallest_ind].fill_(1000000)\n",
    "\n",
    "        term1 = torch.mm(input, means_k.t())\n",
    "        term2 = (input**2).sum(1, keepdim = True)\n",
    "        term3 = (means_k**2).sum(1, keepdim = True).t()\n",
    "        dist = term2 + term3 - 2*term1\n",
    "        dist = rho/2*dist*-1\n",
    "        #print(dist.size())\n",
    "        #print(count_k.size())\n",
    "        #dist = dist + torch.log(count_k[:max_class].unsqueeze(0)+ 1)\n",
    "        batchL = dist.max(1)[1]\n",
    "\n",
    "    \n",
    "    return batchL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.misc\n",
    "eyeMat = Variable(torch.eye(batch_size).cuda())\n",
    "rho = 1/160\n",
    "rho0 = 1/300\n",
    "rho_mean = rho*rho0/(rho+rho0)\n",
    "alpha = 1\n",
    "max_class = 300\n",
    "means_k = torch.zeros(1000,ncl).cuda()\n",
    "#Stores the number of points in each cluster\n",
    "count_k = torch.zeros(1000).cuda()\n",
    "#Stores the empirical mean of the points in each cluster\n",
    "emp_sum_k = torch.zeros(1000,ncl).cuda()\n",
    "#Assignment variable for each cluster\n",
    "embeddings = torch.zeros(60000,ncl).cuda()\n",
    "cmk = torch.zeros(batch_size, ncl).cuda()\n",
    "probs = torch.zeros(1000).cuda()\n",
    "\n",
    "loss_disc = torch.zeros(60000)\n",
    "loss_prior = torch.zeros(60000)\n",
    "trainy = trainy.cuda()\n",
    "shuffle = torch.randperm(60000)\n",
    "assignment_x = torch.zeros(60000).int()\n",
    "for i in range(0,60000):\n",
    "    assignment_x[i] = i%max_class\n",
    "assignment_x = assignment_x.cuda()\n",
    "assignment_x = assignment_x[shuffle.cuda()]\n",
    "count_k1 = count_k.clone()\n",
    "emp_sum_k1 = emp_sum_k.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(0,500):\n",
    "    \n",
    "    begin = time.time()\n",
    "    lr = .0001\n",
    "    \n",
    "        \n",
    "    if epoch==0:\n",
    "        print(trainx.shape)\n",
    "        initialize(trainx[:500]) # data based initialization\n",
    "\n",
    "    if(max_class<20):\n",
    "        break\n",
    "\n",
    "    shuffle = torch.randperm(60000)\n",
    "    trainx = trainx[shuffle]\n",
    "    trainy = trainy[shuffle.cuda()]\n",
    "    assignment_x = assignment_x[shuffle.cuda()]\n",
    "    \n",
    "    numBatches = 0\n",
    "    for i in range(0, trainx.size(0), batch_size):\n",
    "        numBatches +=1\n",
    "        x_unl = trainx[i:i+batch_size]\n",
    "        x_ass = assignment_x[i:i+batch_size].long()\n",
    "        ld, lp = train_classifier(x_unl, x_ass)\n",
    "        loss_disc[epoch] += ld\n",
    "        loss_prior[epoch] += lp\n",
    "        \n",
    "        netD.conv1.weight_g.data.copy_(saved_g1)\n",
    "        netD.conv2.weight_g.data.copy_(saved_g2)\n",
    "        \n",
    "    for i in range(0, trainx.size(0), batch_size):\n",
    "        input = trainx[i:i+batch_size]\n",
    "        input = Variable(input.cuda())\n",
    "        embeddings[i:i+batch_size] = netD(input).data\n",
    "    assignment_x = collapsedGibbs(embeddings, assignment_x.clone())\n",
    "    \n",
    "    \n",
    "    compress()\n",
    "    \n",
    "    print('Evaluating test accuracy')\n",
    "    accuracy = evaluate(assignment_x, trainy)\n",
    "    print(\"Disc loss:%.4f, prior loss:%.4f, purity:%.4f\"%(loss_disc[epoch], loss_prior[epoch], accuracy))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
