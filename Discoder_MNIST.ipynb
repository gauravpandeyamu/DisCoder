{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotting\n",
    "#import cv2\n",
    "from torch.nn.utils import weight_norm\n",
    "import scipy.misc\n",
    "#%matplotlib inline\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "# to prevent opencv from initializing CUDA in workers\n",
    "#torch.randn(8).cuda()\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "def gallery(array, ncols=3):\n",
    "    nindex, height, width, intensity = array.shape\n",
    "    nrows = nindex//ncols\n",
    "    assert nindex == nrows*ncols\n",
    "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
    "    result = (array.reshape((nrows, ncols, height, width, intensity))\n",
    "              .swapaxes(1,2)\n",
    "              .reshape((height*nrows, width*ncols, intensity)))\n",
    "    return result\n",
    "\n",
    "def save_samples():\n",
    "    img_bhwc = netG(noise).data.cpu()[0:100].view(100,1,28,28).expand(100,3,28,28)\n",
    "    img_bhwc = img_bhwc.permute(0,2,3,1).numpy()\n",
    "    array = img_bhwc.copy()\n",
    "    result = gallery(array,10)*.5+.5\n",
    "    scipy.misc.imsave('outfile_noent_1.jpg', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 400\n",
    "learning_rate = .003\n",
    "batch_size = 100\n",
    "unlabeled_weight = 1\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,)  )\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "trainx = train_loader.dataset.train_data.float().view(-1,28*28)/255\n",
    "trainy = train_loader.dataset.train_labels\n",
    "#trainx.add_(-.5).mul_(2);\n",
    "\n",
    "trainx_unl = trainx.clone()\n",
    "trainx_unl2 = trainx.clone()\n",
    "\n",
    "\n",
    "\n",
    "testx = test_loader.dataset.test_data.float().view(-1,28*28)/255\n",
    "testy = test_loader.dataset.test_labels\n",
    "#testx.add_(-.5).mul_(2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_dim = (batch_size, 100)\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.Linear(100, 500, bias=False),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.Linear(500, 500, bias=False),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 8 x 8\n",
    "            nn.Linear(500, 784, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear')!=-1:\n",
    "        m.weight.data.normal_(0.0, 0.1)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    \n",
    "    \n",
    "netG = _netG()\n",
    "netG.cuda()\n",
    "netG.apply(weights_init);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DynamicGNoiseConv(nn.Module):\n",
    "    def __init__(self, shape, std=0.05):\n",
    "        super().__init__()\n",
    "        self.noise = Variable(torch.zeros(1,1,shape,shape).cuda())\n",
    "        self.std   = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not training: return x\n",
    "        self.noise.data.normal_(0, std=self.std)      \n",
    "        return x + self.noise.expand_as(x)\n",
    "\n",
    "class DynamicGNoise(nn.Module):\n",
    "    def __init__(self, shape, std=0.1):\n",
    "        super().__init__()\n",
    "        self.noise = Variable(torch.zeros(batch_size,shape).cuda())\n",
    "        self.std   = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not training: return x\n",
    "        self.noise.data.normal_(0, std=self.std)      \n",
    "        return x + self.noise\n",
    "    \n",
    "    \n",
    "ins = [784,1000,500,250,250,250,10]\n",
    "sigma = [.3,.5,.5,.5,.5,.5]\n",
    "  \n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "\n",
    "        self.linear = [None]*6\n",
    "        self.noise = [None]*6\n",
    "        for i in range(0,6):\n",
    "            self.noise[i] = DynamicGNoise(ins[i], sigma[i])\n",
    "            self.linear[i] = nn.Linear(ins[i],ins[i+1])\n",
    "            self.linear[i].weight.data.normal_(0.0, 0.1)\n",
    "            self.linear[i].bias.data.fill_(0)\n",
    "            self.linear[i] = weight_norm(self.linear[i])\n",
    "        self.linear = nn.ModuleList(self.linear)\n",
    "        self.noise = nn.ModuleList(self.noise)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(0,6):\n",
    "            x = self.noise[i](x)\n",
    "            x = self.linear[i](x)\n",
    "            if(i!=5):\n",
    "                x = F.relu(x)\n",
    "            if(i==3):\n",
    "                features = x.clone()\n",
    "        return x, features\n",
    "\n",
    "netD = _netD()\n",
    "netD.cuda()\n",
    "#netD.apply(weights_init);\n",
    "\n",
    "optimizerC = optim.Adam(netD.parameters(), lr=learning_rate, betas=(.5, .999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(.5, .999))\n",
    "noise = Variable(torch.randn(batch_size,100).cuda())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.cuda()\n",
    "phi_all = torch.rand(3000,10).cuda()\n",
    "phi_all /= phi_all.sum(1).unsqueeze(1).expand_as(phi_all)\n",
    "classMat = torch.eye(10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(x_unl):\n",
    "    global avg, saved_g\n",
    "    netD.eval()\n",
    "    x = Variable(x_unl.cuda())\n",
    "    for i in range(0,5):\n",
    "        x_new = netD.linear[i](x).data\n",
    "        m = x_new.mean(0)\n",
    "        inv_stdv = 1/(x_new**2).mean(0).sqrt().view(-1,1)\n",
    "        netD.linear[i].weight_g.data.copy_(netD.linear[i].weight_g.data*inv_stdv)\n",
    "        netD.linear[i].bias.data.copy_(-m*inv_stdv.squeeze())\n",
    "        x = netD.linear[i](x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "    x_new = netD.linear[5](x).data\n",
    "    m = x_new.mean(0)\n",
    "    inv_stdv = 1/(x_new**2).mean(0).sqrt().view(-1,1)\n",
    "    netD.linear[5].weight_g.data.copy_(netD.linear[5].weight_g.data*inv_stdv)\n",
    "    netD.linear[5].bias.data.copy_(-m*inv_stdv.squeeze())\n",
    "\n",
    "    avg = [None]*18\n",
    "    i = 0\n",
    "    for param in netD.parameters():\n",
    "        avg[i] = param.data.clone()\n",
    "        i += 1\n",
    "\n",
    "    saved_g = [None]*5\n",
    "    for i in range(0,5):\n",
    "        saved_g[i] = netD.linear[i].weight_g.data.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Estep(phi):\n",
    "    if(epoch==0):\n",
    "        px_z = phi/phi.sum(0).expand_as(phi)\n",
    "    else:\n",
    "        px_z = phi/phi_all.sum(0).expand_as(phi)\n",
    "    _, inds = px_z.max(1)\n",
    "    z = classMat.index_select(0, inds.squeeze())\n",
    "    #z = px_z**2\n",
    "    #z /= z.sum(1, keepdim=True).expand_as(z)\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(x_lab, labels, x_unl):\n",
    "    global phi_all\n",
    "    netD.train()\n",
    "    \n",
    "    optimizerC.zero_grad()\n",
    "    labels = Variable(labels.cuda())\n",
    "    x_lab = Variable(x_lab.cuda())\n",
    "    output_before_softmax_lab = netD(x_lab)[0]\n",
    "    loss_lab = loss_fn(output_before_softmax_lab, labels)\n",
    "    \n",
    "    x_unl = Variable(x_unl.cuda())\n",
    "    output_before_softmax_unl = netD(x_unl)[0]\n",
    "    output_after_softmax_unl = F.softmax(output_before_softmax_unl)\n",
    "    phi_all = torch.cat((output_after_softmax_unl.data, phi_all),0)[0:3000]\n",
    "    z = Estep(output_after_softmax_unl.data)\n",
    "    z = Variable(z)\n",
    "    log_phi = torch.log(output_after_softmax_unl+1e-5)\n",
    "    \n",
    "    exponent = torch.mm(z, log_phi.t())\n",
    "    exponent2 = exponent - torch.diag(exponent).view(batch_size,1).expand_as(exponent)\n",
    "    temp = exponent2.exp()\n",
    "    px_z_inv = temp.sum(1)\n",
    "    loss_unl = px_z_inv.log().mean()\n",
    "\n",
    "    \n",
    "    noise.data.normal_(0,1)\n",
    "    gen_data = netG(noise)\n",
    "    output_before_softmax_gen = netD(gen_data.detach())[0]\n",
    "    output_after_softmax_gen = F.softmax(output_before_softmax_gen)\n",
    "    loss_gen = (torch.log(output_after_softmax_gen+1e-5)).mean(1).mean()*-1\n",
    "\n",
    "    loss = loss_lab + loss_unl + loss_gen\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizerC.step()\n",
    "    train_err = (output_before_softmax_lab.data.max(1)[1]==labels.data).sum()/batch_size\n",
    "    return train_err, loss_lab.data[0], loss_unl.data[0], loss_gen.data[0]\n",
    "    \n",
    "\n",
    "def test_classifier(x_test, labels):\n",
    "    netD.eval()\n",
    "    x_test = Variable(x_test.cuda())\n",
    "    output_before_softmax = netD(x_test)[0]\n",
    "    test_err = (output_before_softmax.data.max(1)[1]==labels).sum()/batch_size\n",
    "    return test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator(x_unl):\n",
    "    netD.train()\n",
    "\n",
    "    optimizerG.zero_grad()\n",
    "    x_unl = Variable(x_unl.cuda())\n",
    "    noise.data.normal_(0,1)\n",
    "    gen_data = netG(noise)\n",
    "    output_unl = netD(x_unl)[1]\n",
    "    output_gen = netD(gen_data)[1]\n",
    "    m1 = output_unl.mean(0)\n",
    "    m2 = output_gen.mean(0)\n",
    "    loss_gen = ((m1-m2)**2).mean()\n",
    "    loss_gen.backward()\n",
    "    optimizerG.step()\n",
    "    \n",
    "    return loss_gen.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select labeled data\n",
    "shuffle = torch.randperm(trainx.size(0))\n",
    "trainx = trainx.index_select(0,shuffle)\n",
    "trainy = trainy.index_select(0,shuffle)\n",
    "\n",
    "txs = torch.zeros(100,784)\n",
    "tys = torch.zeros(100)\n",
    "for i in range(0,10):\n",
    "    inds = trainy.eq(i).nonzero()[0:10]\n",
    "    txs[i*10:(i+1)*10] = trainx.index_select(0, inds.squeeze())\n",
    "    tys[i*10:(i+1)*10] = trainy.index_select(0, inds.squeeze())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.misc\n",
    "\n",
    "train_err = torch.zeros(150)\n",
    "loss_lab = torch.zeros(150)\n",
    "loss_unl = torch.zeros(150)\n",
    "loss_gen = torch.zeros(150)\n",
    "lossG_gen = torch.zeros(150)\n",
    "accuracy = torch.zeros(150)\n",
    "\n",
    "scale = 1\n",
    "for epoch in range(0,150):\n",
    "    scale = scale*.99\n",
    "    \n",
    "    begin = time.time()\n",
    "    lr = .003\n",
    "    training = True\n",
    "\n",
    "    for param_group in optimizerC.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    for param_group in optimizerG.param_groups:\n",
    "        param_group['lr'] = lr   \n",
    "    \n",
    "        \n",
    "    trainx = torch.zeros(int(np.ceil(trainx_unl.size(0)/float(txs.size(0))))*txs.size(0),784)\n",
    "    trainy = torch.zeros(int(np.ceil(trainx_unl.size(0)/float(txs.size(0))))*txs.size(0))\n",
    "\n",
    "    for t in range(int(np.ceil(trainx_unl.size(0)/float(txs.size(0))))):\n",
    "        inds = torch.randperm(txs.size(0))\n",
    "        trainx[t*txs.size(0):(t+1)*txs.size(0)] = txs.index_select(0,inds)\n",
    "        trainy[t*txs.size(0):(t+1)*txs.size(0)] = tys.index_select(0,inds)\n",
    "\n",
    "    trainx_unl = trainx_unl[torch.randperm(trainx_unl.size(0))]\n",
    "    trainx_unl2 = trainx_unl2[torch.randperm(trainx_unl2.size(0))]\n",
    "\n",
    "    if epoch==0:\n",
    "        print(trainx.shape)\n",
    "        initialize(trainx[:500]) # data based initialization\n",
    "\n",
    "    \n",
    "    \n",
    "    numBatches = 0\n",
    "    for i in range(0, trainx_unl.size(0), batch_size):\n",
    "        numBatches +=1\n",
    "        x_lab = trainx[i:i+batch_size]\n",
    "        labels = trainy[i:i+batch_size].long()\n",
    "        x_unl = trainx_unl[i:i+batch_size]\n",
    "        te, ll, lu, lg = train_classifier(x_lab, labels, x_unl)\n",
    "        train_err[epoch] += te\n",
    "        loss_lab[epoch] += ll\n",
    "        loss_unl[epoch] += lu\n",
    "        loss_gen[epoch] += lg\n",
    "        \n",
    "        x_unl = trainx_unl2[i:i+batch_size]\n",
    "        lgg = train_generator(x_unl)\n",
    "        lossG_gen[epoch] += lgg\n",
    "        j=0\n",
    "        for param in netD.parameters():\n",
    "            avg[j] = avg[j] + .0001*(param.data - avg[j])\n",
    "            j += 1 \n",
    "        for j in range(0,5):\n",
    "            netD.linear[j].weight_g.data.copy_(saved_g[j])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    j=0\n",
    "    backup = [None]*18\n",
    "    for param in netD.parameters():\n",
    "        backup[j] = param.data.clone()\n",
    "        param.data.copy_(avg[j])\n",
    "        j += 1 \n",
    "\n",
    "    #Computation of test accuracy\n",
    "    posterior = torch.zeros(testx.size(0), 10)\n",
    "    netD.eval()\n",
    "    training = False\n",
    "    for i in range(0, testx.size(0), batch_size):\n",
    "        real_cpu = testx[i:i+batch_size].clone()\n",
    "        input = Variable(real_cpu.cuda())\n",
    "        output = netD(input)[0]\n",
    "        posterior[i:i+batch_size] = output.data.cpu().clone() #Estep(output.data)\n",
    "\n",
    "\n",
    "    j=0\n",
    "    for param in netD.parameters():\n",
    "        param.data.copy_(backup[j])\n",
    "        j += 1 \n",
    "    \n",
    "\n",
    "    _, indices_fake = posterior.cpu().max(1)\n",
    "    indices_fake = indices_fake.squeeze().float()\n",
    "    indices_real = testy\n",
    "    intersect = torch.zeros(10,10)\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,10):\n",
    "            intersect[i][j] = ((indices_fake==i)*(indices_real==j)).sum()\n",
    "\n",
    "    accuracy[epoch] = intersect.max(1)[0].sum()/intersect.sum()\n",
    "    print(\"epoch:%d, loss_lab:%.4f, loss_gen:%.4f, loss_unl:%.4f, lossG_gen:%.4f, train_err:%.4f, test_err:%.4f\" % (epoch, loss_lab[epoch]/numBatches, loss_gen[epoch]/numBatches, \n",
    "          loss_unl[epoch]/numBatches, lossG_gen[epoch]/numBatches, train_err[epoch]/numBatches, accuracy[epoch]))\n",
    "\n",
    "    save_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
